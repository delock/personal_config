#!/home/gma/miniforge3/bin/python
import os
import sys
import json
import signal
import time
from openai import OpenAI
from googlesearch import search
from rich.console import Console
from rich.markdown import Markdown
from rich.live import Live
import requests
from bs4 import BeautifulSoup

# Initialize Rich console
console = Console()

# Environment variables
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_API_URL = os.getenv("OPENAI_API_URL")
OPENAI_API_MODEL = os.getenv("OPENAI_API_MODEL")

# Check if environment variables are set
if not OPENAI_API_KEY or not OPENAI_API_URL or not OPENAI_API_MODEL:
    console.print("[bold red]Error:[/bold red] Environment variables OPENAI_API_KEY, OPENAI_API_URL, and OPENAI_API_MODEL must be set.")
    sys.exit(1)

# Initialize OpenAI client
client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_URL)

def handle_interrupt(signum, frame):
    console.print("\nGoodbye!")
    sys.exit(0)

def query_deepseek(messages, stream=False):
    try:
        response = client.chat.completions.create(
            model=OPENAI_API_MODEL,
            messages=messages,
            stream=stream
        )
        if stream:
            return response
        else:
            return response.choices[0].message.content
    except Exception as e:
        console.print(f"[bold red]Error:[/bold red] {e}")
        sys.exit(1)

def decide_web_search(question):
    messages = [
        {"role": "system", "content": "You are a helpful assistant that decides if a web search is needed. Output in JSON format."},
        {"role": "user", "content": f"Question: {question}\n\nOutput JSON format: {{\"need_search\": true/false, \"keywords\": \"search keywords\"}}"}
    ]
    response = query_deepseek(messages)
    response = response.replace("```json", "").replace("```", "")
    try:
        decision = json.loads(response)
        return decision.get("need_search", False), decision.get("keywords", "")
    except json.JSONDecodeError:
        return False, ""

def pick_search_link(question, search_results):
    references = "\n".join([f"- [{result['title']}]({result['url']}): {result['description']}" for result in search_results])
    messages = [
        {"role": "system", "content": "You are a helpful assistant that decides which search result is most relevant to your question.  Output in JSON format."},
        {"role": "user", "content": f"Question: {question}\nSearch results {references}\n\nOutput JSON format: {{\"url\": url}}"}
    ]
    response = query_deepseek(messages)
    response = response.replace("```json", "").replace("```", "")
    try:
        decision = json.loads(response)
        return decision.get("url", "")
    except json.JSONDecodeError:
        return False, ""

# give web page scrape text, and key words, extract relevant part from text
# according to keywords, using deepseek api
def extract_relevant_text(text, keywords):
    messages = [
        {"role": "system", "content": "You are a helpful assistant that extracts relevant text from a web page. Keep text less than 1000 words. Output in JSON format."},
        {"role": "user", "content": f"Text: {text}\nKeywords: {keywords}\n\nOutput JSON format: {{\"relevant_text\": \"extracted text\"}}"}
    ]
    response = query_deepseek(messages)
    response = response.replace("```json", "").replace("```", "")
    try:
        extraction = json.loads(response)
        return extraction.get("relevant_text", "")
    except json.JSONDecodeError:
        return ""

def strip_empty_lines(text):
    # Split the text into lines, filter out empty lines, and join them back
    return '\n'.join([line for line in text.splitlines() if line.strip()])

def perform_web_search(keywords, question):
    search_results = []
    for result in search(keywords, advanced=True, num_results=20):
        text = result.description
        search_results.append({
            "title": result.title,
            "url": result.url,
            "description": text
        })
    url = pick_search_link(question, search_results)
    try:
        print ("Extracting the most relvant url")
        response = requests.get(url)
        html_content = response.text
        soup = BeautifulSoup(html_content, "html.parser")
        text = strip_empty_lines(soup.get_text())
        #print ("Extracting the most relvant contents")
        #text = extract_relevant_text(text, keywords)
        #print (text)
    except Exception as e:
        text = ""
        pass
    return search_results, text

def beautify_and_stream_output(response):
    markdown_content = ""
    # update live every 1 seconds
    t0 = time.time()
    interval = 1.0
    last_mark = 0
    mark_step = 16
    with Live(Markdown(markdown_content), auto_refresh=False, vertical_overflow="visible") as live:
        for chunk in response:
            if chunk.choices:
                content = chunk.choices[0].delta.content
                markdown_content += content
                if time.time() - t0 > interval:
                    live.update(Markdown(markdown_content), refresh=True)
                    t0 = time.time()
        live.update(Markdown(markdown_content), refresh=True)

def main():
    signal.signal(signal.SIGINT, handle_interrupt)

    question = " ".join(sys.argv[1:])
    if not question:
        console.print("[bold red]Error:[/bold red] No question provided.")
        sys.exit(1)

    need_search, keywords = decide_web_search(question)

    if need_search:
        console.print(f"[bold yellow]Web search needed for keywords:[/bold yellow] {keywords}")
        search_results, text = perform_web_search(keywords, question)
        references = "\n".join([f"- [{result['title']}]({result['url']}): {result['description']}" for result in search_results])
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": f"{question}\n\nReferences:\n{references}\n\nExtracted text:\n{text}"}
        ]
    else:
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": question}
        ]

    console.print("[bold green]Generating answer...[/bold green]")
    response = query_deepseek(messages, stream=True)
    beautify_and_stream_output(response)

if __name__ == "__main__":
    main()
